# SLR Forecasting — Post-Vacation TODO

*Last updated: 2026-02-17*
*Context: MEMORY.md, predictability_analysis.ipynb (48 cells), slr_analysis.py, slr_data_readers.py, slr_projections.py, test_dols.py (19 tests)*

---

## ~~0. Update DOLS to WLS~~ — DONE

**Completed 2026-02-14.** Unified `calibrate_dols()` now accepts optional `gmsl_sigma` for WLS with HAC standard errors. OLS remains the default when `gmsl_sigma=None`.

- [x] Replaced `calibrate_alpha_dols()` and `calibrate_alpha_dols_quadratic()` with single `calibrate_dols(sea_level, temperature, gmsl_sigma=None, saod=None, order=2, n_lags=2, hac_maxlags=None)`
- [x] Uses `sm.WLS(weights=1/sigma**2).fit(cov_type='HAC')` when sigma provided
- [x] Unified `DOLSResult` dataclass supports order 1/2/3 with `physical_coefficients` and `physical_covariance` (regression coefficients = physical coefficients directly, no factorial transform)
- [x] Deprecation wrappers preserve backward compatibility
- [x] Updated `predictability_analysis.ipynb` cells 1, 7, 10, 15 — no more manual `diag([2,1,1])` transform
- [x] Updated `slr_analysis_notebook.ipynb` cell 20

## ~~1. Add Volcanic SAOD Term to DOLS Framework~~ — DONE

**Completed 2026-02-14.** SAOD readers, DOLS integration, and information-criterion testing all implemented.

- [x] `read_glossac(filepath, wavelength=525)` — GloSSAC v2.23 satellite SAOD (1979-2024, cos-lat weighted global mean, 525 nm)
- [x] `read_mauna_loa_transmission(filepath)` — MLO transmission proxy (1958-2025, SAOD = -ln(T))
- [x] `calibrate_dols(..., saod=saod_series)` adds integral-SAOD regressor + SAOD leads/lags
- [x] `test_saod_ic()` compares DOLS with/without SAOD via F-test, AIC, BIC; returns recommendation string
- [x] `test_rate_temperature_nonlinearity()` extended to compare linear, quadratic, AND cubic with pairwise F-tests
- [x] Fixed date alignment bug: series with different day-of-month timestamps (1st vs 15th) normalized to month-start before intersection
- [x] Added SAOD section to `read_process_datafiles.ipynb` (loading, comparison plot, H5 save)
- [x] Added `df_glossac` and `df_mlo_transmission` to H5 store under `raw/`
- [x] Updated `project_gmsl_from_temperature()` and `project_gmsl_ensemble()` in `slr_projections.py` with optional `gamma_saod`/`saod_series`/`saod_scenarios` parameters. SAOD=0 for future projections (no predicted eruptions); observed SAOD can be supplied for hindcasting.
- [x] Assessed whether SAOD changes alpha: **No.** Coefficient shifts are all <0.2 pooled SE on the same time range. SAOD absorbs residual variance without biasing temperature sensitivity. Analysis added to `predictability_analysis.ipynb` cell 9.

### Current SAOD findings
- With annual Frederikse data, `test_saod_ic()` recommends EXCLUDE for both GloSSAC and MLO SAOD (F-test not significant, AIC/BIC favor simpler model)
- Same-time-range coefficient shift test (1979–2018): dα/dT shifts by +31% but only 0.08 pooled SE — well within uncertainty
- γ_saod t-stat = 0.26 (not significant); R² improves by only +0.0003; AIC worsens by +7
- Conclusion: volcanic signal does not alias into α; SAOD can safely be omitted from the baseline DOLS model
- This may change with monthly sea-level data (e.g., satellite altimetry) where volcanic signal is better resolved
- GloSSAC Pinatubo peak SAOD: 0.127 at 1992-06

### Key references
- GloSSAC: Thomason et al. (2018), doi:10.5194/essd-10-469-2018
- Pinatubo SLR effect: Church et al. (2005) noted ~5 mm drop in GMSL post-Pinatubo

---

## ~~2. Run DOLS on IPCC Projections — Emergent Quadratic Sensitivity Test~~ — DONE

**Goal:** Determine whether the quadratic temperature sensitivity (`dα/dT ≈ 3–6 mm/yr/°C²`) that DOLS finds in the observational record is also an emergent property of IPCC AR6/CMIP6 process-model output. Agreement would strongly validate the semi-empirical approach; disagreement would highlight a fundamental data–model divergence worth investigating and discussing.

### Motivation
- DOLS fits a quadratic rate model `dH/dt = a·T² + b·T + c` to *observed* GMSL and GMST. The positive `a` implies an accelerating sensitivity — each additional degree of warming produces more sea-level rise per unit time than the last.
- IPCC AR6 GMSL projections (FACTS framework) are generated by process models (GCMs + ice sheet models + glacier models, etc.), not by fitting to the observational record directly. The projected GMSL and GMST trajectories are available in our H5 store for all five SSPs.
- If fitting the same DOLS model to IPCC-projected (T, H) pairs recovers a similar quadratic coefficient, it means the process models *independently* produce the same emergent nonlinear response — strong evidence that the quadratic structure is physically real, not a statistical artifact.
- If the IPCC projections are better described by a linear model or a different polynomial, that divergence is itself scientifically interesting: it may indicate that the process models handle ice-sheet feedbacks differently than implied by the observational trend.

### What needs to be done
- [ ] **Extract IPCC projected GMSL and GMST for each SSP** from the H5 store (medium confidence). These are time series from ~2015–2150. GMST projections may need to be sourced from CMIP6 ensemble means per SSP (check what's already in `ipcc_conf` or H5 keys).
- [ ] **Fit `calibrate_dols()` to each SSP's (GMSL, GMST) projection** with `order=2`. Since these are smooth model outputs with no observational noise, `n_lags=0` and OLS are appropriate. Record `dalpha_dT`, `alpha0`, `trend`, R² for each SSP.
- [ ] **Fit linear model (`order=1`) for comparison** to test whether the IPCC projections are better described as linear or quadratic. Use `test_rate_temperature_nonlinearity()` on the IPCC rate-vs-T relationship.
- [ ] **Compare IPCC-derived coefficients with observational DOLS:** Tabulate and plot:
  - Observational DOLS OLS: `dalpha_dT` from Frederikse + Berkeley
  - Observational DOLS WLS: same with uncertainty weighting
  - IPCC SSP1-2.6 through SSP5-8.5: `dalpha_dT` from each SSP's projected (T, H)
  - Do the IPCC-derived values bracket the observational estimate? Do higher-forcing SSPs show larger or smaller quadratic sensitivity?
- [ ] **Cross-SSP consistency:** If the quadratic sensitivity is a true physical relationship, `dalpha_dT` should be approximately SSP-independent (since it reflects the physics of ocean heat uptake and ice response, not the forcing pathway). Test this: does `dalpha_dT` vary significantly across SSPs, or is it roughly constant?
- [ ] **Rate-vs-temperature phase diagram:** Plot the IPCC-projected rate vs T for each SSP alongside the observational data and DOLS fit. This is potentially a key publication figure — it would show whether the observed quadratic arc is consistent with where the process models say we're heading.
- [ ] **Decomposed component test (high priority):** Fit DOLS separately to thermodynamic (thermosteric + glaciers) and ice-sheet components of IPCC projections. This is the cleanest test — the thermodynamic component responds continuously to T and should show the strongest quadratic signal, while the ice-sheet component has threshold/tipping behavior and may be linear or poorly described by any polynomial. If the thermodynamic quadratic matches the observational estimate while the ice-sheet component diverges, that directly validates the paper's predictable/unpredictable partition. Component data is already in `ipcc_conf[conf][ssp_code][component]['slc']` (in mm).
- [ ] **Sanity-check with `np.polyfit`:** On smooth model output, `calibrate_dols(n_lags=0)` is really just polynomial regression in integral space. Also fit `np.polyfit(T, rate, 2)` directly on the IPCC rate-vs-T curves — the two methods should agree almost exactly on deterministic trajectories. Any discrepancy flags a methodological issue worth understanding before drawing conclusions.

### Interpretation guide
- **IPCC `dalpha_dT` ≈ observational value (3–6 mm/yr/°C²):** Strong validation. The process models and observations independently show the same nonlinear sensitivity. The DOLS framework is capturing real physics.
- **IPCC `dalpha_dT` significantly lower:** Models may underestimate the acceleration seen in data. Could indicate missing feedbacks or that recent acceleration is partly internal variability.
- **IPCC `dalpha_dT` significantly higher:** Models may project faster acceleration than the historical record supports. Could indicate the observational record is too short to resolve the full nonlinearity.
- **IPCC shows linear, not quadratic:** The nonlinearity in data may come from ice-sheet dynamics or internal variability, not from the smooth thermodynamic response that dominates models. This would be an important finding for the paper.
- **`dalpha_dT` varies strongly across SSPs:** The sensitivity is state-dependent (not just T-dependent), suggesting the quadratic approximation breaks down under very high or very low forcing.
- **Thermodynamic component quadratic but ice-sheet component not:** The strongest possible result for our framing — the predictable part of SLR (thermodynamic) has the same emergent quadratic in both data and models, while the unpredictable part (ice sheets) is where the divergence lives.

### Caveats
- Low-forcing SSPs (SSP1-1.9, SSP1-2.6) have narrow temperature ranges over which the quadratic term is poorly constrained. Expect the most informative fits from SSP3-7.0 and SSP5-8.5.
- If `dalpha_dT` appears SSP-dependent, this may not invalidate the quadratic — it could mean the ice-sheet component (which has threshold crossings that differ across SSPs) is contaminating the total-GMSL fit. The decomposed test would resolve this ambiguity.
- DOLS is designed for cointegrated stochastic series; applying it to smooth deterministic trajectories is valid but is really just integral-space polynomial regression. The `np.polyfit` sanity check guards against over-interpreting the DOLS machinery on data it wasn't designed for.

### Notes
- IPCC GMSL projections are relative to 2005 baseline (same as our DOLS baseline)
- IPCC data is in mm in `ipcc_conf`; need to convert to metres before DOLS
- GMST projections per SSP may come from `ipcc_conf[conf][ssp_code]` or may need to be loaded separately from CMIP6 multi-model means
- This analysis could become Fig 2 or Fig S4 in the manuscript — it's a powerful cross-validation

---

## ~~2b. DOLS Robustness Matrix — Multi-Dataset Sensitivity~~ — DONE

**Completed 2026-02-16.** Runs DOLS on every combination of 7 GMSL datasets × 4 GMST datasets to demonstrate coefficient robustness.

- [x] Script: `notebooks/dols_robustness.py` — `run_analysis(start_year, order, n_lags)`
- [x] GMSL datasets: Frederikse (total), Frederikse thermo (GMSL−TWS), Dangendorf (total), Dangendorf sterodynamic, Horwath thermo (GMSL−TWS), IPCC observed (total), IPCC observed thermo (GMSL−Frederikse TWS)
- [x] GMST datasets: Berkeley Earth, GISTEMP, HadCRUT5, NOAA GlobalTemp
- [x] Produces 7×4 = 28 DOLS fits; Horwath excluded from ensemble (n=20 too few for stable quadratic)
- [x] Combined ensemble: mean ± across-dataset spread (not concatenated series)
- [x] Figures: `dols_robustness_heatmap.png`, `dols_robustness_forest.png`, `dols_robustness_ensemble.png`

### Key findings (start_year=1950, order=2, n_lags=2)
- **Thermodynamic ensemble (8 pairs, excl. Horwath + Dangendorf sterodynamic):** α₀ = 0.49 ± 0.58 mm/yr/°C, dα/dT = 2.85 ± 0.38 mm/yr/°C²
- **All-dataset ensemble (24 pairs, excl. Horwath):** α₀ = 1.73 ± 1.21 mm/yr/°C, dα/dT = 1.83 ± 1.09 mm/yr/°C²
- GMST choice has modest impact (within ±0.5 mm/yr/°C for α₀); GMSL choice dominates spread
- Dangendorf `sterodynamic` shows near-zero dα/dT (~0 ± 0.2), confirming it isolates only the ocean dynamic component (no ice/glacier acceleration signal) — **excluded from thermodynamic ensemble** (see Notes)
- IPCC observed thermodynamic (GMSL − Frederikse TWS) agrees well with Frederikse thermodynamic (dα/dT ≈ 3 mm/yr/°C² for both)
- Horwath (1993–2016, 24 yr) produces wildly unstable estimates; excluded from ensemble statistics

### Notes
- Dangendorf `sterodynamic` ≠ GMSL − TWS: it is the sterodynamic contribution only (thermal expansion + ocean dynamics), which excludes all land ice contributions. This explains its near-zero quadratic sensitivity — the acceleration comes from ice mass loss, not from ocean thermal expansion rate changes.
- IPCC observed GMSL has no component decomposition; TWS removal uses Frederikse TWS interpolated to IPCC years (1950–2018 overlap)
- `start_year` parameter controls data truncation: default 1950 (IPCC obs start); can set earlier for records with longer coverage

---

## ~~2c. Sliding-Window DOLS — Epoch Sensitivity of α₀ and dα/dT~~ — DONE

**Completed 2026-02-17.** Implemented `calibrate_dols_sliding()` in `slr_analysis.py` and created `dols_sliding_window.py` standalone analysis script.

- [x] `SlidingDOLSResult` dataclass with time-varying coefficients, standard errors, R², n_effective, optional gamma_saod
- [x] `calibrate_dols_sliding(sea_level, temperature, gmsl_sigma=None, saod=None, order=2, n_lags=2, span_years=40, kernel='tricube', min_effective_obs=30, hac_maxlags=None)` — kernel-weighted WLS at each center year
- [x] `calibrate_dols_sliding_multibandwidth()` — convenience wrapper for multiple bandwidths
- [x] Multi-bandwidth analysis: h = 30, 40, 50, 60 years
- [x] Cross-dataset comparison at h=40 yr for Frederikse total/thermo, Dangendorf total, IPCC observed total/thermo
- [x] α₀ vs dα/dT tradeoff scatterplot (colored by center year)
- [x] SAOD reconsideration: MLO vs GloSSAC significance in sliding windows
- [x] 8 figures generated in `figures/`

### Key findings
- **α₀–dα/dT tradeoff confirmed**: Quadratic and linear coefficients trade off in a smooth, epoch-dependent manner. The start-date sensitivity from §8.4 is not a statistical artifact — it reflects genuine epoch dependence in the rate-temperature relationship
- **MLO SAOD significant in ~50% of sliding windows** (vs 0% in static DOLS): Volcanic forcing matters for epoch-specific fits, particularly for windows centered on the pre-1980 era that include the major 1963 (Agung) and 1982 (El Chichón) eruptions
- **GloSSAC SAOD significant in only 2–5% of windows**: The shorter GloSSAC record (1979+) limits its coverage to windows that miss the pre-satellite volcanic events
- **Coefficients show secular evolution**: dα/dT increases from ~0–1 in early-century windows to ~3–5 in late-century windows, consistent with accelerating ice-sheet contributions emerging in the thermodynamic signal
- **Cross-dataset agreement**: Different GMSL datasets agree on the temporal structure of coefficient evolution, though absolute values differ

### Scripts and figures
- `notebooks/dols_sliding_window.py` — standalone analysis script
- `figures/dols_sliding_multibw_frederikse.png`, `..._frederikse_thermo.png`, `..._dangendorf.png`, `..._ipcc_observed.png`, `..._ipcc_obs_thermo.png`
- `figures/dols_sliding_cross_dataset.png`
- `figures/dols_sliding_alpha_tradeoff.png`
- `figures/dols_sliding_saod_comparison.png`

---

## 3. Add Dangendorf Thermodynamic Signal  *(was Priority 2)*

**Goal:** Compute a Dangendorf-based thermodynamic signal as a second independent estimate alongside the existing Frederikse-based one.

### Current state
- `compute_thermodynamic_signal()` in `slr_analysis.py` expects columns: `steric`, `glaciers`, `greenland`, `antarctica`, `tws`
- Formula: `thermodynamic_sum = steric + glaciers + greenland + antarctica`, `thermodynamic_gmsl = GMSL - TWS`
- Frederikse (2020) has all required component columns
- **Dangendorf (2024) has `steric` and `barystatic` but NOT the component breakdown** (no separate glaciers, greenland, antarctica columns)

### What needs to be done
- [ ] Determine approach: Since Dangendorf provides `steric + barystatic` but not component-level decomposition, the simplest path is `thermodynamic = steric + barystatic` (which equals `GMSL - TWS - GIA` roughly). Alternatively, use Frederikse component ratios to approximate the Dangendorf decomposition.
- [ ] Option A (simpler): Add a `compute_thermodynamic_signal_dangendorf()` variant that works with `steric` + `barystatic` columns directly
- [ ] Option B: Generalize `compute_thermodynamic_signal()` to detect available columns and compute accordingly
- [ ] Add the Dangendorf thermodynamic signal to `read_process_datafiles.ipynb` (after cell 38 where Frederikse thermodynamic is computed)
- [ ] Include in kinematics computation and visualizations
- [ ] Cross-validate: Compare Frederikse vs Dangendorf thermodynamic signals over their overlapping period (1900-2018). Agreement strengthens the analysis; disagreement needs explanation.

### Notes
- Dangendorf extends to 2021 (vs Frederikse to 2018), giving 3 extra years of thermodynamic signal
- The `read_dangendorf2024_fields()` reader loads `gmsl`, `sterodynamic`, `barystatic`, `gia` from MATLAB files

---

## 4. Stress-Test WAIS Uncertainty Approaches

**Goal:** Rigorously evaluate the four approaches (A1-A4) for constructing sigma_ice and quantify sensitivity to key assumptions.

### Current state
- A1 (rheology), A2 (stochastic amplification), A3 (discrete mixture), A4 (combined, recommended) all implemented in `predictability_analysis.ipynb` cells 26-34
- A4 scenario weights: S1=0.10, S2=0.55, S3=0.25, S4=0.10
- A4 ranges defined for n=3 rheology; rheology correction applied to all; stochastic to MISI scenarios only

### What needs to be done
- [ ] **Sensitivity to scenario weights:** Sweep A4 weights systematically (e.g., shift +/-0.1 on each scenario) and show how sigma_ice at 2100 changes. Plot tornado diagram or response surface.
- [ ] **Sensitivity to scenario ranges:** Perturb the (low, high) bounds of each scenario's contribution range by +/-20%. Which scenario drives the most variance?
- [ ] **Rheology exponent sensitivity:** A1 uses n=3 baseline corrected to n=4. Test n=3.5 and n=4.5. How much does the correction factor change?
- [ ] **Stochastic amplification magnitude:** A2 Robel et al. parameter sensitivity — what if stochastic noise is 50% larger or smaller?
- [ ] **Comparison with published estimates:** Tabulate sigma_ice at 2100 from: (a) IPCC medium confidence, (b) IPCC low confidence, (c) A1, (d) A2, (e) A3, (f) A4, and (g) published values from Bamber et al. (2019), DeConto et al. (2021), Edwards et al. (2021). Are we in the right ballpark?
- [ ] **Tail behavior:** Compare the 99th percentile of A4 with published "worst case" estimates. Does A4 capture the possibility of >2m WAIS contribution?
- [ ] **Internal consistency check:** Verify that A4 samples, when convolved with the DOLS-constrained component, reproduce the IPCC low-confidence total GMSL range (approximately)

---

## 5. Delineate Predictable vs Unpredictable Components

**Goal:** Sharpen the paper's central argument by cleanly separating what DOLS can predict (thermodynamic response + trend) from what it cannot (ice sheet dynamics, volcanic, internal variability), and extend to Greenland sensitivity.

### 5a. Greenland SLR Sensitivity to Regional Warming

**Current state:** Only global mean temperature (GMST) is used. No regional temperature data exists in the project. Greenland is warming ~2-3x the global mean (Arctic amplification).

- [ ] **Acquire Greenland regional temperature data:** Options include:
  - ERA5 reanalysis averaged over Greenland (70-84N, 20-75W)
  - Berkeley Earth regional (land-only Greenland subregion)
  - DMI (Danish Meteorological Institute) station composites
  - GrIS surface mass balance temperature from MAR or RACMO
- [ ] Write reader function for the chosen dataset (with proper `df.attrs`)
- [ ] Fit Greenland ice mass loss (IMBIE data, already loaded) vs regional warming (not GMST)
- [ ] Compare: sensitivity to regional T vs sensitivity to GMST — is Greenland SLR more predictable when using the right temperature?
- [ ] If so, construct a component-level semi-empirical model: `GrIS_rate = alpha_GrIS * T_regional + trend` and fold into the DOLS framework as a separate predictable component
- [ ] Implications for variance decomposition: Does using regional T shrink the "ice sheet deep uncertainty" fraction?

### 5b. Strengthen Predictable/Unpredictable Framing
- [ ] Explicitly quantify what fraction of 2050 and 2100 uncertainty is reducible (more data/better models) vs irreducible (scenario dependence, chaos)
- [ ] Relate the constrained/scenario/ice partition to decision-relevant timescales: short-term planning (2050, mostly predictable) vs long-term adaptation (2100, dominated by deep uncertainty)
- [ ] Connect DOLS residuals to internal climate variability — are residuals white noise or do they show decadal structure (AMO/PDO)?
- [ ] Volcanic contribution: After adding SAOD (item 1), quantify how much variance the volcanic term explains and whether it's "predictable" (if we can't predict future eruptions, it's noise)
- [ ] IPCC cross-validation: Incorporate results from item 2 (DOLS on IPCC projections) to strengthen the argument that the quadratic sensitivity is physically grounded, not a statistical artifact

---

## 5c. Component-Wise DOLS — Per-Component Temperature Sensitivity

**Goal:** Separate and quantify the sensitivity to warming of each SLR component independently: thermosteric, Greenland, glaciers, and Antarctica. This extends the "whole signal" DOLS to ask: which components drive the acceleration?

### Motivation
- DOLS on total GMSL (or GMSL−TWS) gives a single dα/dT, but this is a weighted mix of thermosteric acceleration, glacier acceleration, ice-sheet acceleration, etc.
- Fitting each component separately reveals which components contribute the quadratic term and which are approximately linear in temperature
- This is essential for interpreting the IPCC discrepancy: if IPCC process models have the "right" thermosteric sensitivity but the "wrong" ice-sheet sensitivity, that's a very different conclusion than if everything disagrees

### What needs to be done
- [ ] **Fit DOLS to Frederikse component columns:** `steric`, `glaciers`, `greenland`, `antarctica` each regressed against GMST. Record α₀ and dα/dT for each component.
- [ ] **Compare component sensitivities:** Which components show significant quadratic? Expected: thermosteric and glaciers are continuous T-responses; Greenland may be quadratic due to Arctic amplification; Antarctica may be poorly fit by any polynomial
- [ ] **Replicate with IPCC projected components:** Fit the same component-wise DOLS on IPCC projected `oceandynamics`, `glaciers`, `GIS`, `AIS` to compare model vs observed per-component sensitivities
- [ ] **Cross-check:** Do per-component α₀ values sum to the total GMSL α₀? (They should, approximately, since DOLS is linear in H.)
- [ ] **Variance attribution:** Which component contributes the most to the total dα/dT?
- [ ] **Horwath budget:** Horwath provides independent component estimates (1993-2016) — use for validation even though the short record limits statistical power

### Data requirements
- Frederikse: has `steric`, `glaciers`, `greenland`, `antarctica` (1900-2018) ✓
- IPCC projections: has `oceandynamics`, `glaciers`, `GIS`, `AIS` (decadal, 2020-2100) ✓
- Horwath: has `steric_dieng`, `glaciers`, `greenland`, `antarctica_altimetry`/`antarctica_grace` (monthly, 1993-2016) ✓
- All GMST datasets available ✓

### Notes
- "Thermosteric" in Frederikse is the `steric` column; in IPCC it is `oceandynamics` (which includes dynamic sea-level effects)
- For Antarctica, expect near-zero or noisy α₀ given its small contribution in the observational record (~8-9% of total rate)
- This analysis feeds directly into §7 of the LaTeX document (DOLS vs IPCC sensitivity comparison)

---

## 6. Construct Publication Figures

**Goal:** Create clean, publication-ready figures for the manuscript. Target: Nature-family or AGU journal format.

### Existing figures (in `figures/`)
1. `coefficient_stability.png` — DOLS coefficient stability over training windows
2. `gmslr_projection_histograms.png` — Time-slice KDE distributions at 2050, 2100, 2150
3. `gmslr_projections_comparison.png` — IPCC vs DOLS vs DOLS+WAIS projection envelopes
4. `hindcast_crossvalidation.png` — Leave-future-out cross-validation
5. `ipcc_components.png` — IPCC AR6 component decomposition
6. `ipcc_components_dual_confidence.png` — Medium vs low confidence components
7. `physics_informed_wais_uncertainty.png` — A1-A4 uncertainty comparison
8. `predictability_partition.png` — Variance fraction stacked area (original IPCC)
9. `predictability_partition_physics_informed.png` — Variance fraction with A4

### Planned figure list for manuscript
- [ ] **Fig 1: Observational context** — GMSL records (Frederikse, Dangendorf, altimetry) + GMST, showing the data the model is calibrated on. Include thermodynamic signal overlay. *(Basis: existing `gmsl_gmst_overview.png` in `read_process_datafiles.ipynb`)*
- [ ] **Fig 2: DOLS calibration and hindcast** — (a) Rate vs temperature phase plot with DOLS quadratic fit, (b) hindcast cross-validation showing out-of-sample skill. *(Basis: existing `hindcast_crossvalidation.png` + phase plots)*
- [ ] **Fig 3: Variance decomposition — the core result** — Stacked area showing constrained/scenario/ice fractions from 2020-2150, with and without physics-informed WAIS. Possibly 2-panel (medium vs low confidence) or 3-panel (IPCC / DOLS / DOLS+WAIS). *(Basis: `predictability_partition_physics_informed.png`)*
- [ ] **Fig 4: GMSLR projections** — Projection envelopes for selected SSPs, comparing IPCC/DOLS/DOLS+WAIS. Include 2100 histograms as insets or side panel. *(Basis: `gmslr_projections_comparison.png` + `gmslr_projection_histograms.png`)*
- [ ] **Fig 5: Greenland regional sensitivity** — (a) Greenland mass loss vs regional T, (b) vs GMST for comparison, (c) implication for variance decomposition. *(New — depends on item 5a)*
- [ ] **Fig S1 (supplementary): WAIS uncertainty approaches** — A1-A4 comparison panel. *(Basis: `physics_informed_wais_uncertainty.png`)*
- [ ] **Fig S2: Coefficient stability** — *(Existing: `coefficient_stability.png`)*
- [ ] **Fig S3: IPCC component decomposition** — *(Existing: `ipcc_components_dual_confidence.png`)*
- [ ] **Fig S4: DOLS on IPCC projections** — Rate-vs-temperature phase diagram showing observational data + DOLS fit overlaid with IPCC-projected trajectories for each SSP. Demonstrates whether process models exhibit the same emergent quadratic sensitivity. *(New — depends on item 2)*
- [ ] **Fig S5: Volcanic SAOD effect** — Before/after adding SAOD term to DOLS. *(New — depends on item 1)*
- [ ] **Fig S6: WAIS stress tests** — Tornado/sensitivity diagrams from item 4

### Figure formatting
- [ ] Set up a `visualization_config.py` or cell with consistent styling: Nature-family single-column (89 mm) and double-column (183 mm), 7-8 pt fonts, colorblind-safe palette
- [ ] Ensure all figures save as both PNG (150 dpi for notebook) and PDF (vector for manuscript)
- [ ] Add panel labels (a, b, c) consistently

---

## Implementation Priority

| Priority | Item | Status | Dependencies | Estimated Effort |
|----------|------|--------|-------------|-----------------|
| ~~0~~ | ~~Update DOLS to WLS~~ | **DONE** | None | ~~0.5 day~~ |
| ~~1~~ | ~~Volcanic SAOD in DOLS~~ | **DONE** | None | ~~1-2 days~~ |
| ~~2~~ | ~~DOLS on IPCC projections~~ | **DONE** | IPCC GMST per SSP | ~~1 day~~ |
| ~~2b~~ | ~~DOLS robustness matrix~~ | **DONE** | Item 2 | ~~0.5 day~~ |
| ~~2c~~ | ~~Sliding-window DOLS~~ | **DONE** | Item 2b | ~~1-2 days~~ |
| 3 | Dangendorf thermodynamic | Pending | None (data loaded) | 0.5-1 day |
| 4 | Stress-test WAIS | Pending | None (code exists) | 1-2 days |
| 5a | Greenland regional warming | Pending | New data acquisition | 2-3 days |
| 5b | Predictable/unpredictable framing | Pending | Items 1, 3, 5a | 1-2 days |
| **5c** | **Component-wise DOLS** | **Pending** | Frederikse components | 1 day |
| 6 | Publication figures | Pending | Items 1-5 | 2-3 days |

---

## Recent Completed Work (for context)

**2026-02-17 — Sliding-window DOLS, Dangendorf exclusion, README, notebook documentation:**
- Excluded Dangendorf sterodynamic from thermodynamic ensemble in `dols_robustness.py` — thermodynamic ensemble tightened from dα/dT = 1.90 ± 1.38 to **2.85 ± 0.38** mm/yr/°C²
- Implemented `calibrate_dols_sliding()` and `SlidingDOLSResult` dataclass in `slr_analysis.py` — kernel-weighted WLS (tricube/Gaussian/Epanechnikov) at each center year
- Created `notebooks/dols_sliding_window.py` — standalone analysis script with multi-bandwidth, cross-dataset, and SAOD comparison analyses; 8 figures generated
- Key SAOD finding: MLO SAOD significant in ~50% of sliding windows (vs 0% in static DOLS); GloSSAC only significant in 2–5%
- Created comprehensive `README.md` for GitHub sharing
- Updated all three Jupyter notebooks with descriptive markdown cells: `predictability_analysis.ipynb` (now 56 cells), `read_process_datafiles.ipynb` (53 cells), `slr_analysis_notebook.ipynb` (28 cells)
- Moved 5 deprecated files to `notebooks/archive/`: `old.slr_semiempirical.py`, `unitconv.slr_data_readers.py`, `save.slr_analysis_notebook.ipynb`, `slr_analysis_notebook_executed.ipynb`, `01_data_alignment_and_kinematics.ipynb`

**2026-02-14 — Factorial transform bug fix + verification suite:**
- Found and fixed critical bug in `calibrate_dols()`: integral regressors were divided by `k!` AND regression coefficients multiplied by `k!`, resulting in `(k!)²` inflation. For quadratic (`k=2`), `dalpha_dT` was inflated by **4×**.
- Fix: regressors are now raw `∫T^k` (no factorial scaling); regression coefficients directly equal physical rate-model coefficients (identity mapping, no transform needed).
- Corrected coefficients (Frederikse + Berkeley): `dalpha_dT` OLS = 3.23 ± 1.40 mm/yr/°C² (was 12.07); WLS = 5.88 ± 1.39 mm/yr/°C² (was 20.35). Now consistent with independent `np.polyfit` on finite-difference rates (7.3 mm/yr/°C²).
- Created `test_dols.py` — 19-test publication-quality verification suite: exact recovery on synthetic data (machine precision), noise robustness, WLS correctness, SAOD integration, Monte Carlo unbiasedness, dimensional consistency, n_lags stability, backward compatibility.
- **Notebooks need kernel restart + full re-run** to propagate corrected coefficients through projections, variance decomposition, and figures.

**2026-02-14 — Priority 0 & 1 FULLY COMPLETE (WLS + SAOD):**
- Unified `calibrate_dols()` replacing both old linear/quadratic functions; supports order 1/2/3, optional WLS, optional SAOD
- Unified `DOLSResult` dataclass with `physical_coefficients`/`physical_covariance`
- `test_rate_temperature_nonlinearity()` extended to linear vs quadratic vs cubic with pairwise F-tests, AIC/BIC
- `test_saod_ic()` for AIC/BIC/F-test evaluation of SAOD inclusion
- `read_glossac()` and `read_mauna_loa_transmission()` SAOD readers with `df.attrs` metadata
- Date alignment fix: month-start normalization for series with mismatched day-of-month timestamps
- `project_gmsl_from_temperature()` and `project_gmsl_ensemble()` updated with optional `gamma_saod`/`saod_scenarios` for hindcasting (SAOD=0 default for future projections)
- SAOD aliasing assessment: volcanic forcing does NOT bias α estimates (coefficient shifts <0.2 pooled SE)
- All three notebooks updated: `read_process_datafiles.ipynb` (52 cells), `predictability_analysis.ipynb` (48 cells), `slr_analysis_notebook.ipynb`
- H5 store now includes `raw/df_glossac` and `raw/df_mlo_transmission`

**Earlier work:**
- Added `df.attrs` metadata (units, DOIs, references) to all 14 reader functions
- Created `convert_to_standard_units()` and `convert_units()` in `slr_data_readers.py`
- Unit registry supports: m/mm/cm/ft/in, degC/degF/K, yr/day/month/s, plus compound rates
- Added IPCC low-confidence GMSL reader (`read_ipcc_ar6_projected_gmsl_low_confidence`)
- `read_process_datafiles.ipynb` updated to 52 cells with transparent unit conversion pipeline
- `predictability_analysis.ipynb` updated to load `ipcc_gmsl_low` from H5 and import `convert_units`
- All data saved to `data/processed/slr_processed_data.h5`
