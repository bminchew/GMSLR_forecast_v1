{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Predictable and Unpredictable Components of 21st-Century Sea-Level Rise\n",
    "\n",
    "This notebook implements three core analyses:\n",
    "\n",
    "1. **Variance decomposition** — Partition total SLR projection uncertainty into three additive components at each time horizon: *constrained* (DOLS parameter uncertainty), *scenario* (across SSP pathways), and *ice sheet dynamics* (residual deep uncertainty).\n",
    "\n",
    "2. **Hindcast cross-validation** — Calibrate the DOLS model on truncated records (through 2000, 2005, 2010) and evaluate forecast skill against the withheld observations, demonstrating that the \"predictable\" component is genuinely predictable.\n",
    "\n",
    "3. **Predictability partition figure** — Visualize how the relative contribution of each uncertainty source evolves from 2030 to 2150, showing the transition from a forecastable near-term to a deeply uncertain long-term dominated by ice sheet dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "from scipy import interpolate, stats\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project paths\n",
    "sys.path.insert(0, os.path.dirname(os.path.abspath('__file__')))\n",
    "\n",
    "from slr_analysis import (\n",
    "    calibrate_alpha_dols_quadratic,\n",
    "    resample_to_monthly,\n",
    "    DOLSQuadraticResult\n",
    ")\n",
    "from slr_projections import project_gmsl_ensemble, project_gmsl_from_temperature\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Plot style\n",
    "plt.rcParams.update({\n",
    "    'figure.dpi': 150,\n",
    "    'font.size': 10,\n",
    "    'axes.labelsize': 11,\n",
    "    'axes.titlesize': 12,\n",
    "    'legend.fontsize': 9,\n",
    "    'figure.facecolor': 'white',\n",
    "    'axes.facecolor': 'white',\n",
    "    'axes.grid': True,\n",
    "    'grid.alpha': 0.3,\n",
    "})\n",
    "\n",
    "# Conversion factor\n",
    "M_TO_MM = 1000.0\n",
    "\n",
    "print('Imports loaded successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Load all data from H5 file\n",
    "# ============================================================\n",
    "h5_path = '../data/processed/slr_processed_data.h5'\n",
    "\n",
    "with pd.HDFStore(h5_path, 'r') as store:\n",
    "    # Configuration\n",
    "    config = store['/config']\n",
    "    baseline_start = int(config['baseline_start'].iloc[0])\n",
    "    baseline_end = int(config['baseline_end'].iloc[0])\n",
    "    \n",
    "    # Harmonized observations (baseline = 1995-2005)\n",
    "    df_frederikse = store['/harmonized/df_frederikse_h']\n",
    "    df_berkeley = store['/harmonized/df_berkeley_h']\n",
    "    df_imbie_wais = store['/harmonized/df_imbie_wais_h']\n",
    "    \n",
    "    # Temperature projections (IPCC AR6)\n",
    "    temp_hist = store['/projections/temp/Historical']\n",
    "    temp_projections = {}\n",
    "    ssp_keys = {\n",
    "        'SSP1-1.9': 'SSP1_1_9', 'SSP1-2.6': 'SSP1_2_6',\n",
    "        'SSP2-4.5': 'SSP2_4_5', 'SSP3-7.0': 'SSP3_7_0',\n",
    "        'SSP5-8.5': 'SSP5_8_5'\n",
    "    }\n",
    "    for name, key in ssp_keys.items():\n",
    "        temp_projections[name] = store[f'/projections/temp/{key}']\n",
    "    \n",
    "    # IPCC GMSL projections (with components)\n",
    "    ipcc_gmsl = {}\n",
    "    ssp_gmsl_keys = {\n",
    "        'SSP1-1.9': 'ssp119', 'SSP1-2.6': 'ssp126',\n",
    "        'SSP2-4.5': 'ssp245', 'SSP3-7.0': 'ssp370',\n",
    "        'SSP5-8.5': 'ssp585'\n",
    "    }\n",
    "    for name, key in ssp_gmsl_keys.items():\n",
    "        ipcc_gmsl[name] = store[f'/projections/gmsl/{key}']\n",
    "\n",
    "print(f'Baseline: {baseline_start}-{baseline_end}')\n",
    "print(f'Frederikse GMSL: {df_frederikse.index[0].year}-{df_frederikse.index[-1].year} ({len(df_frederikse)} obs)')\n",
    "print(f'Berkeley Earth: {df_berkeley.index[0].year}-{df_berkeley.index[-1].year} ({len(df_berkeley)} obs)')\n",
    "print(f'IMBIE WAIS: {df_imbie_wais.index[0].year}-{df_imbie_wais.index[-1].year} ({len(df_imbie_wais)} obs)')\n",
    "print(f'Temperature scenarios: {list(temp_projections.keys())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DOLS Calibration — Full Record\n",
    "\n",
    "Fit the quadratic semi-empirical model on the full available record to establish reference coefficients:\n",
    "\n",
    "$$\\text{rate} = \\frac{d\\alpha}{dT} \\cdot T^2 + \\alpha_0 \\cdot T + \\beta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Prepare aligned sea level and temperature for calibration\n",
    "# ============================================================\n",
    "# Resample annual Frederikse to monthly for alignment with Berkeley\n",
    "df_fred_monthly = resample_to_monthly(\n",
    "    df_frederikse, value_col='gmsl', unc_col='gmsl_sigma'\n",
    ")\n",
    "\n",
    "# Align on common time range\n",
    "common_idx = df_fred_monthly.index.intersection(df_berkeley.index)\n",
    "sl_series = df_fred_monthly.loc[common_idx, 'gmsl']\n",
    "temp_series = df_berkeley.loc[common_idx, 'temperature']\n",
    "\n",
    "# Full-record calibration\n",
    "result_full = calibrate_alpha_dols_quadratic(\n",
    "    sea_level=sl_series,\n",
    "    temperature=temp_series,\n",
    "    n_lags=2\n",
    ")\n",
    "\n",
    "print('=== Full-Record DOLS Quadratic Calibration ===')\n",
    "print(result_full)\n",
    "print(f'\\nRate model (m/yr):')\n",
    "print(f'  rate = {result_full.dalpha_dT:.6f} T² + {result_full.alpha0:.6f} T + {result_full.trend:.6f}')\n",
    "print(f'  rate = {result_full.dalpha_dT*M_TO_MM:.3f} T² + {result_full.alpha0*M_TO_MM:.3f} T + {result_full.trend*M_TO_MM:.3f}  [mm/yr]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Monte Carlo Ensemble Projections\n",
    "\n",
    "Generate forward projections under all five SSP scenarios, propagating coefficient uncertainty through Monte Carlo sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Build continuous temperature time series for each SSP\n",
    "# (Historical + SSP, merged on overlap)\n",
    "# ============================================================\n",
    "def build_full_temperature_scenario(temp_hist, temp_ssp, baseline_start, baseline_end):\n",
    "    \"\"\"Combine historical and SSP temperature projections into a single series,\n",
    "    re-baselined to the project baseline period.\"\"\"\n",
    "    # Use SSP data from 2015 onward, historical before\n",
    "    hist_part = temp_hist[temp_hist['decimal_year'] < 2015].copy()\n",
    "    combined = pd.concat([hist_part, temp_ssp]).sort_index()\n",
    "    # Remove any duplicates\n",
    "    combined = combined[~combined.index.duplicated(keep='last')]\n",
    "    return combined\n",
    "\n",
    "full_temp_scenarios = {}\n",
    "for name, temp_ssp in temp_projections.items():\n",
    "    full_temp_scenarios[name] = build_full_temperature_scenario(\n",
    "        temp_hist, temp_ssp, baseline_start, baseline_end\n",
    "    )\n",
    "\n",
    "print(f'Temperature scenario time ranges:')\n",
    "for name, df in full_temp_scenarios.items():\n",
    "    print(f'  {name}: {df.decimal_year.min():.0f}-{df.decimal_year.max():.0f} ({len(df)} years)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Run ensemble projections\n",
    "# ============================================================\n",
    "N_SAMPLES = 2000\n",
    "BASELINE_YEAR = 2005.0  # IPCC AR6 reference year\n",
    "\n",
    "# Extract coefficients in [a, b, c] form for projection: rate = a*T² + b*T + c\n",
    "# Note: the covariance matrix from DOLS is for [coeff(∫T²), coeff(∫T), trend, ...]\n",
    "# where coeff(∫T²) = dalpha_dT / 2\n",
    "# We need to transform to get covariance of [dalpha_dT, alpha0, trend]\n",
    "coeffs_abc = np.array([\n",
    "    result_full.dalpha_dT,   # a = dα/dT\n",
    "    result_full.alpha0,       # b = α₀\n",
    "    result_full.trend         # c = β\n",
    "])\n",
    "\n",
    "# Transform covariance: dalpha_dT = 2 * coeff[0]\n",
    "# So var(dalpha_dT) = 4 * var(coeff[0])\n",
    "# cov(dalpha_dT, x) = 2 * cov(coeff[0], x)\n",
    "cov_raw = result_full.covariance[:3, :3]  # first 3 params\n",
    "transform = np.diag([2.0, 1.0, 1.0])  # dalpha_dT = 2 * coeff[0]\n",
    "cov_abc = transform @ cov_raw @ transform.T\n",
    "\n",
    "print(f'Projection coefficients [a, b, c]:')\n",
    "print(f'  a (dα/dT)  = {coeffs_abc[0]*M_TO_MM:.4f} ± {np.sqrt(cov_abc[0,0])*M_TO_MM:.4f} mm/yr/°C²')\n",
    "print(f'  b (α₀)    = {coeffs_abc[1]*M_TO_MM:.4f} ± {np.sqrt(cov_abc[1,1])*M_TO_MM:.4f} mm/yr/°C')\n",
    "print(f'  c (trend)  = {coeffs_abc[2]*M_TO_MM:.4f} ± {np.sqrt(cov_abc[2,2])*M_TO_MM:.4f} mm/yr')\n",
    "\n",
    "# Run ensemble\n",
    "ensemble_results = project_gmsl_ensemble(\n",
    "    coefficients=coeffs_abc,\n",
    "    coefficients_cov=cov_abc,\n",
    "    temperature_projections=full_temp_scenarios,\n",
    "    baseline_year=BASELINE_YEAR,\n",
    "    baseline_gmsl=0.0,\n",
    "    n_samples=N_SAMPLES,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f'\\nEnsemble projections complete ({N_SAMPLES} samples per scenario).')\n",
    "for name, df in ensemble_results['scenarios'].items():\n",
    "    row_2100 = df[df['decimal_year'] >= 2099].iloc[0] if len(df[df['decimal_year'] >= 2099]) > 0 else None\n",
    "    if row_2100 is not None:\n",
    "        print(f'  {name} at ~2100: {row_2100[\"gmsl\"]*M_TO_MM:.0f} [{row_2100[\"gmsl_lower\"]*M_TO_MM:.0f}, {row_2100[\"gmsl_upper\"]*M_TO_MM:.0f}] mm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Variance Decomposition\n",
    "\n",
    "Decompose total SLR projection uncertainty into three components at each time horizon:\n",
    "\n",
    "- **$\\sigma^2_{\\text{constrained}}$** : From DOLS coefficient uncertainty, propagated forward for a *fixed* SSP (we use the across-scenario mean to isolate parameter uncertainty)\n",
    "- **$\\sigma^2_{\\text{scenario}}$** : Variance of median projections *across* SSP pathways (reflects societal choice)\n",
    "- **$\\sigma^2_{\\text{ice}}$** : Residual variance between our observationally-constrained projection and the full IPCC range, dominated by ice sheet dynamics\n",
    "\n",
    "The decomposition is computed at decadal intervals matching IPCC projection time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4a. Run full Monte Carlo to get per-scenario uncertainty \n",
    "#     at each time step (not just median/5-95)\n",
    "# ============================================================\n",
    "\n",
    "def run_full_ensemble(coefficients, cov, temp_scenarios, baseline_year=2005.0,\n",
    "                      n_samples=2000, seed=42):\n",
    "    \"\"\"\n",
    "    Run Monte Carlo ensemble and return full sample arrays.\n",
    "    Returns dict of {scenario: {'time': array, 'gmsl_samples': (n_samples, n_time)}}\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    coeff_samples = rng.multivariate_normal(coefficients, cov, n_samples)\n",
    "    \n",
    "    results = {}\n",
    "    for sname, temp_df in temp_scenarios.items():\n",
    "        T = temp_df['temperature'].values\n",
    "        time_years = temp_df['decimal_year'].values\n",
    "        dt = np.diff(time_years)\n",
    "        baseline_idx = np.argmin(np.abs(time_years - baseline_year))\n",
    "        nt = len(T)\n",
    "        \n",
    "        gmsl_samples = np.zeros((n_samples, nt))\n",
    "        rate_samples = np.zeros((n_samples, nt))\n",
    "        \n",
    "        for k in range(n_samples):\n",
    "            a, b, c = coeff_samples[k]\n",
    "            rate = a * T**2 + b * T + c\n",
    "            rate_samples[k] = rate\n",
    "            \n",
    "            gmsl = np.zeros(nt)\n",
    "            for i in range(baseline_idx, nt - 1):\n",
    "                gmsl[i+1] = gmsl[i] + 0.5 * (rate[i] + rate[i+1]) * dt[i]\n",
    "            for i in range(baseline_idx, 0, -1):\n",
    "                gmsl[i-1] = gmsl[i] - 0.5 * (rate[i] + rate[i-1]) * dt[i-1]\n",
    "            gmsl_samples[k] = gmsl\n",
    "        \n",
    "        results[sname] = {\n",
    "            'time': time_years,\n",
    "            'gmsl_samples': gmsl_samples,\n",
    "            'rate_samples': rate_samples,\n",
    "            'gmsl_median': np.median(gmsl_samples, axis=0),\n",
    "            'gmsl_p5': np.percentile(gmsl_samples, 5, axis=0),\n",
    "            'gmsl_p95': np.percentile(gmsl_samples, 95, axis=0),\n",
    "            'gmsl_p17': np.percentile(gmsl_samples, 17, axis=0),\n",
    "            'gmsl_p83': np.percentile(gmsl_samples, 83, axis=0),\n",
    "        }\n",
    "    \n",
    "    return results, coeff_samples\n",
    "\n",
    "mc_results, coeff_samples = run_full_ensemble(\n",
    "    coeffs_abc, cov_abc, full_temp_scenarios, \n",
    "    baseline_year=BASELINE_YEAR, n_samples=N_SAMPLES, seed=42\n",
    ")\n",
    "\n",
    "print('Full Monte Carlo ensemble complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4b. Compute variance decomposition at decadal intervals\n",
    "# ============================================================\n",
    "\n",
    "# Target years for decomposition (matching IPCC decadal steps)\n",
    "target_years = np.arange(2020, 2160, 10)\n",
    "\n",
    "# We use 5 main SSPs for scenario spread\n",
    "scenario_names = ['SSP1-1.9', 'SSP1-2.6', 'SSP2-4.5', 'SSP3-7.0', 'SSP5-8.5']\n",
    "\n",
    "decomposition = []\n",
    "\n",
    "for yr in target_years:\n",
    "    # ---- σ²_constrained: mean coefficient uncertainty across scenarios ----\n",
    "    # For each scenario, get the variance of GMSL at this year across MC samples\n",
    "    var_constrained_per_ssp = []\n",
    "    median_per_ssp = []\n",
    "    \n",
    "    for sname in scenario_names:\n",
    "        mc = mc_results[sname]\n",
    "        time = mc['time']\n",
    "        idx = np.argmin(np.abs(time - yr))\n",
    "        \n",
    "        if np.abs(time[idx] - yr) > 1.0:\n",
    "            continue\n",
    "        \n",
    "        gmsl_at_yr = mc['gmsl_samples'][:, idx]\n",
    "        var_constrained_per_ssp.append(np.var(gmsl_at_yr))\n",
    "        median_per_ssp.append(np.median(gmsl_at_yr))\n",
    "    \n",
    "    if len(median_per_ssp) == 0:\n",
    "        continue\n",
    "    \n",
    "    # σ²_constrained = average variance across scenarios\n",
    "    # (parameter uncertainty is roughly the same regardless of scenario)\n",
    "    sigma2_constrained = np.mean(var_constrained_per_ssp)\n",
    "    \n",
    "    # ---- σ²_scenario: variance of medians across SSPs ----\n",
    "    sigma2_scenario = np.var(median_per_ssp)\n",
    "    \n",
    "    # ---- σ²_ice: residual between DOLS range and IPCC full range ----\n",
    "    # For each SSP, compare DOLS 5-95% range with IPCC 5-95% range\n",
    "    # The excess IPCC variance is attributed to ice dynamics + other processes\n",
    "    # not captured by the DOLS semi-empirical model\n",
    "    \n",
    "    # IPCC total variance (from 5-95% range, approximated as 1.645σ for each tail)\n",
    "    ipcc_var_per_ssp = []\n",
    "    dols_var_per_ssp = []\n",
    "    \n",
    "    for sname in scenario_names:\n",
    "        # IPCC data\n",
    "        ipcc_key = sname\n",
    "        if ipcc_key in ipcc_gmsl:\n",
    "            ipcc_df = ipcc_gmsl[ipcc_key]\n",
    "            ipcc_row = ipcc_df[np.abs(ipcc_df['decimal_year'] - yr) < 1.0]\n",
    "            if len(ipcc_row) > 0:\n",
    "                ipcc_lower = ipcc_row['gmsl_lower'].values[0]\n",
    "                ipcc_upper = ipcc_row['gmsl_upper'].values[0]\n",
    "                # 5-95% range → σ ≈ range / (2 × 1.645)\n",
    "                ipcc_sigma = (ipcc_upper - ipcc_lower) / (2 * 1.645)\n",
    "                ipcc_var_per_ssp.append(ipcc_sigma**2)\n",
    "        \n",
    "        # DOLS variance for this scenario\n",
    "        mc = mc_results[sname]\n",
    "        idx = np.argmin(np.abs(mc['time'] - yr))\n",
    "        if np.abs(mc['time'][idx] - yr) < 1.0:\n",
    "            dols_var_per_ssp.append(np.var(mc['gmsl_samples'][:, idx]))\n",
    "    \n",
    "    # σ²_ice = mean(IPCC variance) - σ²_constrained - σ²_scenario\n",
    "    # This represents the additional uncertainty in IPCC projections not\n",
    "    # captured by the semi-empirical model (dominated by ice sheet dynamics)\n",
    "    if len(ipcc_var_per_ssp) > 0:\n",
    "        # Average IPCC total variance across SSPs\n",
    "        sigma2_ipcc_total = np.mean(ipcc_var_per_ssp)\n",
    "        sigma2_ice = max(0, sigma2_ipcc_total - sigma2_constrained)\n",
    "    else:\n",
    "        sigma2_ice = np.nan\n",
    "    \n",
    "    # Also store IPCC component-level info at this year\n",
    "    ais_values = []\n",
    "    for sname in scenario_names:\n",
    "        if sname in ipcc_gmsl:\n",
    "            ipcc_df = ipcc_gmsl[sname]\n",
    "            row = ipcc_df[np.abs(ipcc_df['decimal_year'] - yr) < 1.0]\n",
    "            if len(row) > 0 and 'AIS' in row.columns:\n",
    "                ais_values.append(row['AIS'].values[0])\n",
    "    \n",
    "    decomposition.append({\n",
    "        'year': yr,\n",
    "        'sigma2_constrained': sigma2_constrained,\n",
    "        'sigma2_scenario': sigma2_scenario,\n",
    "        'sigma2_ice': sigma2_ice,\n",
    "        'sigma2_total': sigma2_constrained + sigma2_scenario + sigma2_ice,\n",
    "        'median_gmsl_range': median_per_ssp,\n",
    "        'ais_median': np.mean(ais_values) if ais_values else np.nan,\n",
    "    })\n",
    "\n",
    "df_decomp = pd.DataFrame(decomposition)\n",
    "\n",
    "# Convert to mm for display\n",
    "for col in ['sigma2_constrained', 'sigma2_scenario', 'sigma2_ice', 'sigma2_total']:\n",
    "    df_decomp[f'{col}_mm2'] = df_decomp[col] * M_TO_MM**2\n",
    "\n",
    "# Compute fractional contributions\n",
    "for col in ['sigma2_constrained', 'sigma2_scenario', 'sigma2_ice']:\n",
    "    df_decomp[f'frac_{col}'] = df_decomp[col] / df_decomp['sigma2_total']\n",
    "\n",
    "print('=== Variance Decomposition ===')\n",
    "print(f'{\"Year\":>6} {\"σ_const (mm)\":>14} {\"σ_scen (mm)\":>14} {\"σ_ice (mm)\":>14} {\"σ_total (mm)\":>14} {\"f_const\":>8} {\"f_scen\":>8} {\"f_ice\":>8}')\n",
    "print('-' * 100)\n",
    "for _, row in df_decomp.iterrows():\n",
    "    yr = int(row['year'])\n",
    "    if yr <= 2150:\n",
    "        sc = np.sqrt(row['sigma2_constrained_mm2'])\n",
    "        ss = np.sqrt(row['sigma2_scenario_mm2'])\n",
    "        si = np.sqrt(row['sigma2_ice_mm2']) if not np.isnan(row['sigma2_ice_mm2']) else 0\n",
    "        st = np.sqrt(row['sigma2_total_mm2'])\n",
    "        fc = row['frac_sigma2_constrained']\n",
    "        fs = row['frac_sigma2_scenario']\n",
    "        fi = row['frac_sigma2_ice']\n",
    "        print(f'{yr:>6} {sc:>14.1f} {ss:>14.1f} {si:>14.1f} {st:>14.1f} {fc:>8.1%} {fs:>8.1%} {fi:>8.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hindcast Cross-Validation\n",
    "\n",
    "To demonstrate that the \"predictable\" component is genuinely predictable, we calibrate the DOLS model on truncated historical records and evaluate out-of-sample forecast skill.\n",
    "\n",
    "**Protocol:**\n",
    "1. Calibrate on data through year $Y_{\\text{cut}}$ (using $Y_{\\text{cut}}$ ∈ {2000, 2005, 2010})\n",
    "2. Project forward using *observed* temperatures (not SSP scenarios) from Berkeley Earth\n",
    "3. Compare projected GMSL against observed GMSL in the withheld period\n",
    "4. Evaluate: bias, RMSE, coverage of 90% prediction interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5a. Hindcast calibration and evaluation\n",
    "# ============================================================\n",
    "\n",
    "cutoff_years = [2000, 2005, 2010]\n",
    "hindcast_results = {}\n",
    "\n",
    "for cut_yr in cutoff_years:\n",
    "    # Truncate calibration data\n",
    "    mask_cal = sl_series.index.year <= cut_yr\n",
    "    sl_cal = sl_series[mask_cal]\n",
    "    temp_cal = temp_series[mask_cal]\n",
    "    \n",
    "    if len(sl_cal) < 100:  # need reasonable calibration period\n",
    "        print(f'Skipping cutoff {cut_yr}: insufficient data ({len(sl_cal)} obs)')\n",
    "        continue\n",
    "    \n",
    "    # Calibrate on truncated record\n",
    "    try:\n",
    "        result_trunc = calibrate_alpha_dols_quadratic(\n",
    "            sea_level=sl_cal,\n",
    "            temperature=temp_cal,\n",
    "            n_lags=2\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f'Calibration failed for cutoff {cut_yr}: {e}')\n",
    "        continue\n",
    "    \n",
    "    # Build coefficients for projection\n",
    "    coeffs_trunc = np.array([\n",
    "        result_trunc.dalpha_dT,\n",
    "        result_trunc.alpha0,\n",
    "        result_trunc.trend\n",
    "    ])\n",
    "    cov_raw_trunc = result_trunc.covariance[:3, :3]\n",
    "    transform = np.diag([2.0, 1.0, 1.0])\n",
    "    cov_trunc = transform @ cov_raw_trunc @ transform.T\n",
    "    \n",
    "    # Use OBSERVED temperatures for the full period (not SSP projections)\n",
    "    # Build a temperature DataFrame covering 1950 through end of Berkeley record\n",
    "    temp_for_proj = df_berkeley[['temperature']].copy()\n",
    "    temp_for_proj['decimal_year'] = [\n",
    "        t.year + (t.month - 1) / 12 + (t.day - 1) / 365.25\n",
    "        for t in temp_for_proj.index\n",
    "    ]\n",
    "    # Resample to annual for cleaner projection\n",
    "    temp_annual = temp_for_proj.resample('YS').mean()\n",
    "    temp_annual['decimal_year'] = [\n",
    "        t.year + 0.5 for t in temp_annual.index\n",
    "    ]\n",
    "    \n",
    "    # Run ensemble projection with observed temps\n",
    "    mc_hind = run_full_ensemble(\n",
    "        coeffs_trunc, cov_trunc,\n",
    "        {'observed': temp_annual},\n",
    "        baseline_year=BASELINE_YEAR,\n",
    "        n_samples=N_SAMPLES, seed=42\n",
    "    )[0]\n",
    "    \n",
    "    # Extract projected GMSL\n",
    "    hind_time = mc_hind['observed']['time']\n",
    "    hind_median = mc_hind['observed']['gmsl_median']\n",
    "    hind_p5 = mc_hind['observed']['gmsl_p5']\n",
    "    hind_p95 = mc_hind['observed']['gmsl_p95']\n",
    "    \n",
    "    # Get observed GMSL for comparison (Frederikse, annual)\n",
    "    fred_time = df_frederikse['year'].values if 'year' in df_frederikse.columns else np.array([\n",
    "        t.year + 0.5 for t in df_frederikse.index\n",
    "    ])\n",
    "    fred_gmsl = df_frederikse['gmsl'].values\n",
    "    \n",
    "    # Align Frederikse observations to IPCC baseline (2005)\n",
    "    # Find the value at 2005 in Frederikse and subtract it\n",
    "    idx_2005_fred = np.argmin(np.abs(fred_time - 2005.0))\n",
    "    fred_gmsl_rebase = fred_gmsl - fred_gmsl[idx_2005_fred]\n",
    "    \n",
    "    # Compute verification metrics for withheld period\n",
    "    mask_verify = (fred_time > cut_yr) & (fred_time <= 2018)\n",
    "    if mask_verify.sum() > 0:\n",
    "        verify_time = fred_time[mask_verify]\n",
    "        verify_obs = fred_gmsl_rebase[mask_verify]\n",
    "        \n",
    "        # Interpolate projection to observation times\n",
    "        proj_at_obs = np.interp(verify_time, hind_time, hind_median)\n",
    "        proj_p5_at_obs = np.interp(verify_time, hind_time, hind_p5)\n",
    "        proj_p95_at_obs = np.interp(verify_time, hind_time, hind_p95)\n",
    "        \n",
    "        # Metrics\n",
    "        bias = np.mean(proj_at_obs - verify_obs) * M_TO_MM\n",
    "        rmse = np.sqrt(np.mean((proj_at_obs - verify_obs)**2)) * M_TO_MM\n",
    "        coverage = np.mean((verify_obs >= proj_p5_at_obs) & (verify_obs <= proj_p95_at_obs))\n",
    "    else:\n",
    "        bias = rmse = coverage = np.nan\n",
    "    \n",
    "    hindcast_results[cut_yr] = {\n",
    "        'result': result_trunc,\n",
    "        'coeffs': coeffs_trunc,\n",
    "        'cov': cov_trunc,\n",
    "        'time': hind_time,\n",
    "        'median': hind_median,\n",
    "        'p5': hind_p5,\n",
    "        'p95': hind_p95,\n",
    "        'p17': mc_hind['observed']['gmsl_p17'],\n",
    "        'p83': mc_hind['observed']['gmsl_p83'],\n",
    "        'obs_time': fred_time,\n",
    "        'obs_gmsl': fred_gmsl_rebase,\n",
    "        'bias_mm': bias,\n",
    "        'rmse_mm': rmse,\n",
    "        'coverage_90': coverage,\n",
    "        'cutoff': cut_yr,\n",
    "        'n_cal_years': int(cut_yr - fred_time[0]),\n",
    "    }\n",
    "\n",
    "print('=== Hindcast Cross-Validation Results ===')\n",
    "print(f'{\"Cutoff\":>8} {\"Cal years\":>10} {\"Bias (mm)\":>10} {\"RMSE (mm)\":>11} {\"90% Cov\":>9}')\n",
    "print('-' * 55)\n",
    "for cut_yr, hr in hindcast_results.items():\n",
    "    print(f'{cut_yr:>8} {hr[\"n_cal_years\"]:>10} {hr[\"bias_mm\"]:>10.2f} {hr[\"rmse_mm\"]:>11.2f} {hr[\"coverage_90\"]:>9.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5b. Check coefficient convergence across calibration windows\n",
    "# ============================================================\n",
    "\n",
    "print('=== Coefficient Convergence ===')\n",
    "print(f'{\"Cutoff\":>8} {\"dα/dT (mm/yr/°C²)\":>20} {\"α₀ (mm/yr/°C)\":>18} {\"trend (mm/yr)\":>16}')\n",
    "print('-' * 70)\n",
    "for cut_yr, hr in hindcast_results.items():\n",
    "    r = hr['result']\n",
    "    print(f'{cut_yr:>8} {r.dalpha_dT*M_TO_MM:>12.3f} ± {r.dalpha_dT_se*M_TO_MM:.3f} {r.alpha0*M_TO_MM:>10.3f} ± {r.alpha0_se*M_TO_MM:.3f} {r.trend*M_TO_MM:>8.3f} ± {r.trend_se*M_TO_MM:.3f}')\n",
    "# Full record\n",
    "print(f'{\"Full\":>8} {result_full.dalpha_dT*M_TO_MM:>12.3f} ± {result_full.dalpha_dT_se*M_TO_MM:.3f} {result_full.alpha0*M_TO_MM:>10.3f} ± {result_full.alpha0_se*M_TO_MM:.3f} {result_full.trend*M_TO_MM:>8.3f} ± {result_full.trend_se*M_TO_MM:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hindcast Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6. Hindcast cross-validation figure\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4.5), sharey=True)\n",
    "\n",
    "colors_hind = {2000: '#2166ac', 2005: '#4393c3', 2010: '#92c5de'}\n",
    "\n",
    "for i, (cut_yr, hr) in enumerate(hindcast_results.items()):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    time = hr['time']\n",
    "    \n",
    "    # 90% prediction interval\n",
    "    ax.fill_between(time, hr['p5'] * M_TO_MM, hr['p95'] * M_TO_MM,\n",
    "                    alpha=0.15, color=colors_hind[cut_yr], label='90% PI')\n",
    "    # 66% prediction interval\n",
    "    ax.fill_between(time, hr['p17'] * M_TO_MM, hr['p83'] * M_TO_MM,\n",
    "                    alpha=0.3, color=colors_hind[cut_yr], label='66% PI')\n",
    "    # Median projection\n",
    "    ax.plot(time, hr['median'] * M_TO_MM, '-', color=colors_hind[cut_yr],\n",
    "            lw=2, label='Median projection')\n",
    "    \n",
    "    # Observations\n",
    "    mask_cal = hr['obs_time'] <= cut_yr\n",
    "    mask_ver = hr['obs_time'] > cut_yr\n",
    "    ax.plot(hr['obs_time'][mask_cal], hr['obs_gmsl'][mask_cal] * M_TO_MM,\n",
    "            'o', color='#333333', ms=2, alpha=0.5, label='Calibration obs')\n",
    "    ax.plot(hr['obs_time'][mask_ver], hr['obs_gmsl'][mask_ver] * M_TO_MM,\n",
    "            's', color='#d62728', ms=3, alpha=0.8, label='Withheld obs')\n",
    "    \n",
    "    # Cutoff line\n",
    "    ax.axvline(cut_yr, color='gray', ls='--', lw=1, alpha=0.7)\n",
    "    \n",
    "    # Annotations\n",
    "    ax.set_title(f'Calibration through {cut_yr}', fontweight='bold')\n",
    "    ax.set_xlabel('Year')\n",
    "    ax.set_xlim(1950, 2025)\n",
    "    \n",
    "    # Metrics box\n",
    "    textstr = (f'Bias: {hr[\"bias_mm\"]:.1f} mm\\n'\n",
    "               f'RMSE: {hr[\"rmse_mm\"]:.1f} mm\\n'\n",
    "               f'90% cov: {hr[\"coverage_90\"]:.0%}')\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)\n",
    "    ax.text(0.03, 0.97, textstr, transform=ax.transAxes, fontsize=8,\n",
    "            verticalalignment='top', bbox=props)\n",
    "    \n",
    "    if i == 0:\n",
    "        ax.set_ylabel('GMSL relative to 2005 (mm)')\n",
    "        ax.legend(loc='lower left', fontsize=7)\n",
    "\n",
    "fig.suptitle('Hindcast Cross-Validation: DOLS Semi-Empirical Model', \n",
    "             fontsize=13, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/hindcast_crossvalidation.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved: ../figures/hindcast_crossvalidation.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. The Key Figure: Predictability Partition\n",
    "\n",
    "This figure shows how the relative contribution of each uncertainty source evolves with projection horizon. It is the central result of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7a. Prepare data for the predictability partition figure\n",
    "# ============================================================\n",
    "\n",
    "# Limit to years where we have IPCC data (up to 2150)\n",
    "df_plot = df_decomp[df_decomp['year'] <= 2150].copy()\n",
    "\n",
    "# Compute 1-sigma equivalents in mm for each component\n",
    "df_plot['sigma_constrained_mm'] = np.sqrt(df_plot['sigma2_constrained']) * M_TO_MM\n",
    "df_plot['sigma_scenario_mm'] = np.sqrt(df_plot['sigma2_scenario']) * M_TO_MM\n",
    "df_plot['sigma_ice_mm'] = np.sqrt(df_plot['sigma2_ice'].fillna(0)) * M_TO_MM\n",
    "df_plot['sigma_total_mm'] = np.sqrt(df_plot['sigma2_total']) * M_TO_MM\n",
    "\n",
    "print('Data prepared for figure.')\n",
    "print(df_plot[['year', 'sigma_constrained_mm', 'sigma_scenario_mm', 'sigma_ice_mm', 'sigma_total_mm']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7b. Create the main predictability partition figure\n",
    "# ============================================================\n",
    "\n",
    "fig = plt.figure(figsize=(12, 9))\n",
    "gs = gridspec.GridSpec(2, 2, height_ratios=[1.3, 1], hspace=0.35, wspace=0.3)\n",
    "\n",
    "# --- Panel A: Stacked area showing variance fractions ---\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "\n",
    "years = df_plot['year'].values\n",
    "f_const = df_plot['frac_sigma2_constrained'].values\n",
    "f_scen = df_plot['frac_sigma2_scenario'].values\n",
    "f_ice = df_plot['frac_sigma2_ice'].fillna(0).values\n",
    "\n",
    "# Smooth for visual clarity\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "# Just use raw values since we have decadal data\n",
    "\n",
    "ax1.fill_between(years, 0, f_const, \n",
    "                 color='#2166ac', alpha=0.8, label='Constrained (DOLS coefficients)')\n",
    "ax1.fill_between(years, f_const, f_const + f_scen,\n",
    "                 color='#fdae61', alpha=0.8, label='Scenario (SSP spread)')\n",
    "ax1.fill_between(years, f_const + f_scen, f_const + f_scen + f_ice,\n",
    "                 color='#d73027', alpha=0.8, label='Deep uncertainty (ice dynamics)')\n",
    "\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.set_xlim(2020, 2150)\n",
    "ax1.set_ylabel('Fraction of total variance')\n",
    "ax1.set_xlabel('Year')\n",
    "ax1.set_title('(a) Variance fraction by source', fontweight='bold')\n",
    "ax1.legend(loc='upper left', fontsize=8, framealpha=0.9)\n",
    "ax1.axvline(2050, color='gray', ls=':', lw=0.8, alpha=0.5)\n",
    "ax1.axvline(2100, color='gray', ls=':', lw=0.8, alpha=0.5)\n",
    "\n",
    "# --- Panel B: Absolute 1σ uncertainty by component ---\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "\n",
    "ax2.fill_between(years, 0, df_plot['sigma_constrained_mm'].values,\n",
    "                 color='#2166ac', alpha=0.6)\n",
    "ax2.fill_between(years, df_plot['sigma_constrained_mm'].values,\n",
    "                 df_plot['sigma_constrained_mm'].values + df_plot['sigma_scenario_mm'].values,\n",
    "                 color='#fdae61', alpha=0.6)\n",
    "ax2.fill_between(years,\n",
    "                 df_plot['sigma_constrained_mm'].values + df_plot['sigma_scenario_mm'].values,\n",
    "                 df_plot['sigma_constrained_mm'].values + df_plot['sigma_scenario_mm'].values + df_plot['sigma_ice_mm'].values,\n",
    "                 color='#d73027', alpha=0.6)\n",
    "\n",
    "ax2.set_xlim(2020, 2150)\n",
    "ax2.set_ylabel('1σ uncertainty (mm)')\n",
    "ax2.set_xlabel('Year')\n",
    "ax2.set_title('(b) Absolute uncertainty by source', fontweight='bold')\n",
    "ax2.axvline(2050, color='gray', ls=':', lw=0.8, alpha=0.5)\n",
    "ax2.axvline(2100, color='gray', ls=':', lw=0.8, alpha=0.5)\n",
    "\n",
    "# --- Panel C: DOLS projections vs IPCC ---\n",
    "ax3 = fig.add_subplot(gs[1, :])\n",
    "\n",
    "ssp_colors = {\n",
    "    'SSP1-1.9': '#1b7837', 'SSP1-2.6': '#5aae61',\n",
    "    'SSP2-4.5': '#fee08b', 'SSP3-7.0': '#fc8d59',\n",
    "    'SSP5-8.5': '#d73027'\n",
    "}\n",
    "\n",
    "for sname in scenario_names:\n",
    "    mc = mc_results[sname]\n",
    "    color = ssp_colors[sname]\n",
    "    \n",
    "    # DOLS projection\n",
    "    mask = mc['time'] >= 2005\n",
    "    t = mc['time'][mask]\n",
    "    ax3.fill_between(t, mc['gmsl_p5'][mask] * M_TO_MM, mc['gmsl_p95'][mask] * M_TO_MM,\n",
    "                     alpha=0.12, color=color)\n",
    "    ax3.plot(t, mc['gmsl_median'][mask] * M_TO_MM, '-', color=color, lw=1.5, label=f'{sname} (DOLS)')\n",
    "    \n",
    "    # IPCC AR6 medians and ranges\n",
    "    ipcc_df = ipcc_gmsl[sname]\n",
    "    ipcc_t = ipcc_df['decimal_year'].values\n",
    "    ax3.plot(ipcc_t, ipcc_df['gmsl'].values * M_TO_MM, 's', color=color, ms=4, alpha=0.7)\n",
    "    ax3.errorbar(ipcc_t, ipcc_df['gmsl'].values * M_TO_MM,\n",
    "                 yerr=[(ipcc_df['gmsl'].values - ipcc_df['gmsl_lower'].values) * M_TO_MM,\n",
    "                       (ipcc_df['gmsl_upper'].values - ipcc_df['gmsl'].values) * M_TO_MM],\n",
    "                 fmt='none', color=color, alpha=0.3, capsize=2)\n",
    "\n",
    "# Observations\n",
    "fred_time_plot = df_frederikse['year'].values if 'year' in df_frederikse.columns else np.array([\n",
    "    t.year + 0.5 for t in df_frederikse.index\n",
    "])\n",
    "idx_2005 = np.argmin(np.abs(fred_time_plot - 2005.0))\n",
    "fred_rebase = (df_frederikse['gmsl'].values - df_frederikse['gmsl'].values[idx_2005]) * M_TO_MM\n",
    "ax3.plot(fred_time_plot, fred_rebase, 'k-', lw=1.5, alpha=0.6, label='Observed (Frederikse)')\n",
    "\n",
    "ax3.set_xlim(1990, 2150)\n",
    "ax3.set_ylim(-50, 1000)\n",
    "ax3.set_xlabel('Year')\n",
    "ax3.set_ylabel('GMSL relative to 2005 (mm)')\n",
    "ax3.set_title('(c) DOLS semi-empirical projections vs IPCC AR6 (lines = DOLS, squares = IPCC)', fontweight='bold')\n",
    "ax3.legend(ncol=3, fontsize=7, loc='upper left', framealpha=0.9)\n",
    "ax3.axvline(2050, color='gray', ls=':', lw=0.8, alpha=0.5)\n",
    "ax3.axvline(2100, color='gray', ls=':', lw=0.8, alpha=0.5)\n",
    "\n",
    "fig.suptitle('Predictability of 21st-Century Sea-Level Rise', \n",
    "             fontsize=14, fontweight='bold', y=1.01)\n",
    "\n",
    "plt.savefig('../figures/predictability_partition.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved: ../figures/predictability_partition.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. IPCC Component Breakdown — Where Deep Uncertainty Lives\n",
    "\n",
    "Examine the IPCC AR6 component-level projections to identify which processes drive the deep uncertainty residual. The AIS contribution is nearly SSP-independent, confirming it represents a fundamentally different type of uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 8. IPCC component analysis\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "# Panel A: Component contributions by SSP at 2100\n",
    "ax = axes[0]\n",
    "components = ['oceandynamics', 'AIS', 'GIS', 'glaciers', 'landwaterstorage']\n",
    "comp_labels = ['Ocean\\ndynamics', 'AIS', 'GIS', 'Glaciers', 'Land water\\nstorage']\n",
    "comp_colors = ['#2166ac', '#d73027', '#f4a582', '#92c5de', '#b2abd2']\n",
    "\n",
    "x = np.arange(len(scenario_names))\n",
    "width = 0.15\n",
    "\n",
    "for j, (comp, clabel, color) in enumerate(zip(components, comp_labels, comp_colors)):\n",
    "    vals = []\n",
    "    for sname in scenario_names:\n",
    "        row_2100 = ipcc_gmsl[sname][np.abs(ipcc_gmsl[sname]['decimal_year'] - 2100) < 1]\n",
    "        vals.append(row_2100[comp].values[0] * M_TO_MM if len(row_2100) > 0 else 0)\n",
    "    ax.bar(x + j * width, vals, width, label=clabel, color=color, alpha=0.85)\n",
    "\n",
    "ax.set_xticks(x + 2 * width)\n",
    "ax.set_xticklabels([s.replace('SSP', '') for s in scenario_names], fontsize=9)\n",
    "ax.set_ylabel('Contribution at 2100 (mm rel. to 2005)')\n",
    "ax.set_title('(a) IPCC AR6 GMSL components at 2100', fontweight='bold')\n",
    "ax.legend(fontsize=8, ncol=2)\n",
    "\n",
    "# Panel B: AIS contribution over time — nearly SSP-independent\n",
    "ax = axes[1]\n",
    "for sname in scenario_names:\n",
    "    ipcc_df = ipcc_gmsl[sname]\n",
    "    mask = ipcc_df['decimal_year'] <= 2150\n",
    "    ax.plot(ipcc_df.loc[mask, 'decimal_year'], ipcc_df.loc[mask, 'AIS'] * M_TO_MM,\n",
    "            'o-', color=ssp_colors[sname], ms=4, label=sname)\n",
    "\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('AIS median contribution (mm)')\n",
    "ax.set_title('(b) AIS contribution: nearly SSP-independent', fontweight='bold')\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/ipcc_components.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved: ../figures/ipcc_components.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary Statistics and Key Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9. Summary table of key results\n",
    "# ============================================================\n",
    "\n",
    "print('=' * 80)\n",
    "print('SUMMARY: Predictability of 21st-Century Sea-Level Rise')\n",
    "print('=' * 80)\n",
    "\n",
    "print('\\n--- DOLS Calibration (Full Record) ---')\n",
    "print(f'  dα/dT = {result_full.dalpha_dT*M_TO_MM:.3f} ± {result_full.dalpha_dT_se*M_TO_MM:.3f} mm/yr/°C²')\n",
    "print(f'  α₀    = {result_full.alpha0*M_TO_MM:.3f} ± {result_full.alpha0_se*M_TO_MM:.3f} mm/yr/°C')\n",
    "print(f'  trend = {result_full.trend*M_TO_MM:.3f} ± {result_full.trend_se*M_TO_MM:.3f} mm/yr')\n",
    "print(f'  R²    = {result_full.r2:.4f}')\n",
    "\n",
    "print('\\n--- Variance Decomposition at Key Horizons ---')\n",
    "for yr in [2050, 2100, 2150]:\n",
    "    row = df_decomp[df_decomp['year'] == yr]\n",
    "    if len(row) > 0:\n",
    "        row = row.iloc[0]\n",
    "        print(f'\\n  {yr}:')\n",
    "        print(f'    Constrained (coefficient):  {row[\"frac_sigma2_constrained\"]:.0%} of total variance')\n",
    "        print(f'    Scenario (SSP spread):      {row[\"frac_sigma2_scenario\"]:.0%} of total variance')\n",
    "        print(f'    Deep uncertainty (ice):      {row[\"frac_sigma2_ice\"]:.0%} of total variance')\n",
    "        print(f'    Total 1σ:                   {np.sqrt(row[\"sigma2_total\"])*M_TO_MM:.0f} mm')\n",
    "\n",
    "print('\\n--- Hindcast Skill ---')\n",
    "for cut_yr, hr in hindcast_results.items():\n",
    "    print(f'  Cal through {cut_yr}: bias={hr[\"bias_mm\"]:.1f} mm, RMSE={hr[\"rmse_mm\"]:.1f} mm, 90% coverage={hr[\"coverage_90\"]:.0%}')\n",
    "\n",
    "print('\\n--- Key Messages ---')\n",
    "row_2050 = df_decomp[df_decomp['year'] == 2050].iloc[0]\n",
    "row_2100 = df_decomp[df_decomp['year'] == 2100].iloc[0]\n",
    "print(f'  1. Near-term SLR (to 2050) is dominated by constrained uncertainty')\n",
    "print(f'     ({row_2050[\"frac_sigma2_constrained\"]:.0%} of variance from observable-constrained parameters).')\n",
    "print(f'  2. By 2100, deep uncertainty from ice sheet dynamics contributes')\n",
    "print(f'     {row_2100[\"frac_sigma2_ice\"]:.0%} of total variance — this cannot be reduced by')\n",
    "print(f'     longer global temperature or sea level records.')\n",
    "print(f'  3. The DOLS model demonstrates genuine out-of-sample predictive skill')\n",
    "print(f'     (RMSE < {max(hr[\"rmse_mm\"] for hr in hindcast_results.values()):.0f} mm across all hindcast windows).')\n",
    "print(f'  4. AIS contribution at 2100 ({row_2100[\"ais_median\"]*M_TO_MM:.0f} mm) is nearly SSP-independent,')\n",
    "print(f'     confirming it represents a structurally different type of uncertainty.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 10. Supplementary: coefficient stability plot\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(13, 4))\n",
    "\n",
    "param_names = ['dα/dT', 'α₀', 'trend']\n",
    "param_units = ['mm/yr/°C²', 'mm/yr/°C', 'mm/yr']\n",
    "\n",
    "for j, (pname, punit) in enumerate(zip(param_names, param_units)):\n",
    "    ax = axes[j]\n",
    "    \n",
    "    cut_years_list = sorted(hindcast_results.keys())\n",
    "    x_positions = list(range(len(cut_years_list))) + [len(cut_years_list)]\n",
    "    x_labels = [str(yr) for yr in cut_years_list] + ['Full']\n",
    "    \n",
    "    vals = []\n",
    "    errs = []\n",
    "    for cut_yr in cut_years_list:\n",
    "        r = hindcast_results[cut_yr]['result']\n",
    "        if j == 0:\n",
    "            vals.append(r.dalpha_dT * M_TO_MM)\n",
    "            errs.append(r.dalpha_dT_se * M_TO_MM)\n",
    "        elif j == 1:\n",
    "            vals.append(r.alpha0 * M_TO_MM)\n",
    "            errs.append(r.alpha0_se * M_TO_MM)\n",
    "        else:\n",
    "            vals.append(r.trend * M_TO_MM)\n",
    "            errs.append(r.trend_se * M_TO_MM)\n",
    "    \n",
    "    # Full record\n",
    "    if j == 0:\n",
    "        vals.append(result_full.dalpha_dT * M_TO_MM)\n",
    "        errs.append(result_full.dalpha_dT_se * M_TO_MM)\n",
    "    elif j == 1:\n",
    "        vals.append(result_full.alpha0 * M_TO_MM)\n",
    "        errs.append(result_full.alpha0_se * M_TO_MM)\n",
    "    else:\n",
    "        vals.append(result_full.trend * M_TO_MM)\n",
    "        errs.append(result_full.trend_se * M_TO_MM)\n",
    "    \n",
    "    ax.errorbar(x_positions, vals, yerr=[e * 1.96 for e in errs], \n",
    "                fmt='o', color='#2166ac', capsize=5, ms=7)\n",
    "    \n",
    "    # Highlight full-record value\n",
    "    ax.axhline(vals[-1], color='#d73027', ls='--', lw=1, alpha=0.5)\n",
    "    \n",
    "    ax.set_xticks(x_positions)\n",
    "    ax.set_xticklabels(x_labels)\n",
    "    ax.set_xlabel('Calibration window end')\n",
    "    ax.set_ylabel(f'{pname} ({punit})')\n",
    "    ax.set_title(f'{pname}', fontweight='bold')\n",
    "\n",
    "fig.suptitle('Coefficient Stability Across Calibration Windows (95% CI)',\n",
    "             fontsize=12, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/coefficient_stability.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved: ../figures/coefficient_stability.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
